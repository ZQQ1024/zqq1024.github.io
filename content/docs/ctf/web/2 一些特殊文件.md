
---
title: "一些特殊文件" 
weight: 3
bookToc: true
---

整理一些特殊文件的用途：
- robots.txt
- .swp
- .DS_Store
- .user.ini
- .git/HEAD
- .htaccess
- phps

## robots.txt

robots.txt 是一个放置在网站根目录下的文本文件，它用于告诉搜索引爬虫哪些页面可以抓取，哪些不可以。限制不是强制的，一些爬虫可能会忽视这些规则。
```txt
User-agent: Googlebot
Disallow: /private/
Allow: /public/

User-agent: Baiduspider
Disallow: 

User-agent: *
Disallow: /
```
`Disallow` 规则的优先级高于 `Allow`，以上例子指示：
- `Googlebot` 不允许抓取 `/private/` 目录下的任何内容，允许抓取 `/public/` 目录下的所有内容
-  `Baiduspider` 可以抓取网站上的所有内容
- 对于不是 `Googlebot` 或 `Baiduspider` 的所有其他爬虫，禁止它们访问网站的任何部分

{{< hint info >}}
robots.txt 在 ctf web题目中的作用是提供目录扫描工具扫描不到的网站路径
{{< /hint >}}

{{< expand "题目 攻防世界-Web-robots" "..." >}}
TODO
{{< /expand >}}

## .swp

